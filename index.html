<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="./img/favicon.ico">

        <title>Cloudbreak</title>
				
				<link href="./css/sequenceiq.doc.min.css" rel="stylesheet">
        
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href=".">Cloudbreak</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li class="disabled">
                    <a rel="next" >
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li class="disabled">
                    <a rel="prev" >
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#cloudbreak">Cloudbreak</a></li>
        
            <li><a href="#overview">Overview</a></li>
        
            <li><a href="#benefits">Benefits</a></li>
        
            <li><a href="#quickstart-and-installation">Quickstart and installation</a></li>
        
            <li><a href="#running-cloudbreak-api-using-docker">Running Cloudbreak API using Docker</a></li>
        
            <li><a href="#running-cloudbreak-api-on-the-host">Running Cloudbreak API on the host</a></li>
        
            <li><a href="#configuration">Configuration</a></li>
        
            <li><a href="#how-it-works">How it works?</a></li>
        
            <li><a href="#technology">Technology</a></li>
        
            <li><a href="#supported-components">Supported components</a></li>
        
            <li><a href="#accounts">Accounts</a></li>
        
            <li><a href="#cloudbreak-ui">Cloudbreak UI</a></li>
        
            <li><a href="#add-new-cloud-providers">Add new cloud providers</a></li>
        
            <li><a href="#releases">Releases</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<!--main.md-->

<h1 id="cloudbreak">Cloudbreak</h1>
<p><em>Cloudbreak is a powerful left surf that breaks over a coral reef, a mile off southwest the island of Tavarua, Fiji.</em></p>
<p><em>Cloudbreak is a cloud agnostic Hadoop as a Service API. Abstracts the provisioning and ease management and monitoring of on-demand clusters.</em></p>
<p>http://docs.cloudbreak.apiary.io/</p>
<!--main.md-->

<!--overview.md-->

<h2 id="overview">Overview</h2>
<p>Cloudbreak is a RESTful Hadoop as a Service API. Once it is deployed in your favorite servlet container exposes a REST API allowing to span up Hadoop clusters of arbitary sizes on your selected cloud provider. Provisioning Hadoop has never been easier.
Cloudbreak is built on the foundation of cloud providers API (Amazon AWS, Microsoft Azure, Google Cloud Compute...), Apache Ambari, Docker containers, Serf and dnsmasq.</p>
<h2 id="benefits">Benefits</h2>
<h3 id="secure">Secure</h3>
<p>Supports basic, token based and OAuth2 authentication model. The cluster is provisioned in a logically isolated network (Virtual Private Cloud) of your favorite cloud provider.
Cloudbreak does not store or manages your cloud credentials - it is the end user's responsability to link the Cloudbreak user with her/his cloud account. We provide utilities to ease this process (IAM on Amazon, certificates on Azure).</p>
<h3 id="elastic">Elastic</h3>
<p>Using Cloudbreak API you can provision an arbitrary number of Hadoop nodes - the API does the hard work for you, and span up the infrastructure, configure the network and the selected Hadoop components and services without any user interaction.
POST once and use it anytime after.</p>
<h3 id="scalable">Scalable</h3>
<p>As your workload changes, the API allows you to add or remove nodes on the fly. Cloudbreak does the hard work of reconfiguring the infrastructure, provision or decomission Hadoop nodes and let the cluster be continuosely operational.
Once provisioned, new nodes will take up the load and increase the cluster throughput.</p>
<h3 id="declarative-hadoop-clusters">Declarative Hadoop clusters</h3>
<p>We support declarative Hadoop cluster creation - using blueprints. Blueprints are a declarative definition of a Hadoop cluster. With a blueprint, you specify a stack, the component layout and the configurations to materialize a Hadoop cluster instance. Hostgroups defined in blueprints can be associated to different VPC subnets and availability zones, thus you can span up a highly available cluster running on different datacenters or availability zones.</p>
<h3 id="flexible">Flexible</h3>
<p>You have the option to choose your favorite cloud provider and their different pricing models. The API translated the calls towards different vendors - you develop and use one common API, no need to rewrite your code when changing between cloud providers.</p>
<!--overview.md-->

<!--quickstart.md-->

<h2 id="quickstart-and-installation">Quickstart and installation</h2>
<h2 id="running-cloudbreak-api-using-docker">Running Cloudbreak API using Docker</h2>
<h3 id="database">Database</h3>
<p>The only dependency that Cloudbreak needs is a postgresql database. The easiest way to spin up a postgresql is of course Docker. Run it first with this line:</p>
<pre class="prettyprint well"><code>docker run -d --name=&quot;postgresql&quot; -p 5432:5432 -v /tmp/data:/data -e USER=&quot;seqadmin&quot; -e DB=&quot;cloudbreak&quot; -e PASS=&quot;seq123_&quot; paintedfox/postgresql
</code></pre>

<h3 id="cloudbreak-rest-api">Cloudbreak REST API</h3>
<p>After postgresql is running, Cloudbreak can be started locally in a Docker container with the following command. By linking the database container, the necessary environment variables for the connection are set. The postgresql address can be set explicitly through the environment variable: DB_PORT_5432_TCP_ADDR.</p>
<pre class="prettyprint well"><code>VERSION=0.1-20140623140412

docker run -d --name cloudbreak \
-e &quot;VERSION=$VERSION&quot; \
-e &quot;AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID&quot; \
-e &quot;AWS_SECRET_KEY=$AWS_SECRET_KEY&quot; \
-e &quot;HBM2DDL_STRATEGY=create&quot; \
-e &quot;MAIL_SENDER_USERNAME=$MAIL_SENDER_USERNAME&quot; \
-e &quot;MAIL_SENDER_PASSWORD=$MAIL_SENDER_PASSWORD&quot; \
-e &quot;MAIL_SENDER_HOST=$MAIL_SENDER_HOST&quot; \
-e &quot;MAIL_SENDER_PORT=$MAIL_SENDER_PORT&quot; \
-e &quot;MAIL_SENDER_FROM=$MAIL_SENDER_FROM&quot; \
-e &quot;HOST_ADDR=$HOST_ADDR&quot; \
--link postgresql:db -p 8080:8080 \
dockerfile/java bash \
-c 'curl -o /tmp/cloudbreak-$VERSION.jar https://s3-eu-west-1.amazonaws.com/seq-repo/releases/com/sequenceiq/cloudbreak/$VERSION/cloudbreak-$VERSION.jar &amp;&amp; java -jar /tmp/cloudbreak-$VERSION.jar'

</code></pre>

<p>Note: The system properties prefixed with MAIL_SENDER_ are the SNMP settings required to send emails.  </p>
<h2 id="running-cloudbreak-api-on-the-host">Running Cloudbreak API on the host</h2>
<p>If you'd like to run Cloudbreak outside of a Docker container - directly on the host - we provide you an installation shell script.</p>
<p>After building the application <em>(./gradlew clean build)</em> please run the following script from the project root:</p>
<pre class="prettyprint well"><code>./run_cloudbreak.sh &lt;db-user&gt; &lt;db-pass&gt; &lt;db-host&gt; &lt;db-port&gt; &lt;host-address&gt;
</code></pre>

<p>The arguments are as follows:</p>
<p><code>db-user</code> - your database user</p>
<p><code>db-pass</code> - your password for the database</p>
<p><code>db-host</code> - the address of the machine hosting your database</p>
<p><code>db-port</code> - the port where you can connect to the database</p>
<p><code>host-address</code> - the ngrok generated address to receive SNS notifications</p>
<h2 id="configuration">Configuration</h2>
<h3 id="development">Development</h3>
<p>In order to be able to receive Amazon push notifications on localhost, you will need to install a secure introspectable tunnel to localhost.</p>
<h3 id="install-and-configure-ngrok">Install and configure ngrok</h3>
<p>Cloudbreak uses SNS to receive notifications. On OSX you can do the following:</p>
<pre class="prettyprint well"><code>brew update &amp;&amp; brew install ngrok
ngrok 8080
</code></pre>

<p><em>Note: In the terminal window you'll find displayed a value - this is the last argument <code>host-address</code> of the <code>run_cloudbreak.sh</code> script</em></p>
<h3 id="production">Production</h3>
<p>TBD - add properties list !!!</p>
<!--quickstart.md-->

<!--howitworks.md-->

<h2 id="how-it-works">How it works?</h2>
<p>Cloudbreak launches on-demand Hadoop clusters on your favorite cloud provider in minutes. We have introduced 4 main notions - the core building block of the REST API.</p>
<h3 id="templates">Templates</h3>
<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related resources, maintaining and updating them in an orderly and predictable fashion.
Templates are cloud specific - and on top of the infrastructural setup they collect the information such as the used machine images, the datacenter location, instance types, SSH setup and can capture and control region-specific infrastructure variations.</p>
<p>A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>
<p>The infrastructure specific configuration is available under the Cloudbreak <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/vpc-and-subnet.template">resources</a>.
As an example, for Amazon EC2, we use <a href="http://aws.amazon.com/cloudformation/">AWS Cloudformation</a> to define the cloud infrastructure .</p>
<p>For further information please visit our <a href="http://docs.cloudbreak.apiary.io/#templates">API documentation</a>.</p>
<h3 id="stacks">Stacks</h3>
<p>Stacks are template <code>instances</code> - a runnig cloud infrastructure created based on a template. Stacks are always launched on behalf of a cloud user account. Stacks supports a wide range of resources, allowing you to build a highly available, reliable, and scalable infrastructure for your application needs.</p>
<p>For further information please visit our <a href="http://docs.cloudbreak.apiary.io/#stacks">API documentation</a>.</p>
<h3 id="blueprints">Blueprints</h3>
<p>Ambari Blueprints are a declarative definition of a Hadoop cluster. With a Blueprint, you specify a stack, the component layout and the configurations to materialize a Hadoop cluster instance. Hostgroups defined in blueprints can be associated to different VPC subnets and availability zones, thus you can span up a highly available cluster running on different datacenters or availability zones.
We have a few default blueprints available from single note to multi node blueprints and lamba architecture.</p>
<p>For further information please visit our <a href="http://docs.cloudbreak.apiary.io/#blueprints">API documentation</a>.</p>
<h3 id="cluster">Cluster</h3>
<p>Clusters are materialized Hadoop clusters. They are built based on a Bluerint (running the components and services specified) and on a configured infrastructure Stack.
Once a cluster is created and launched it can be used the usual way as any Hadoop cluster. We suggest to start with the Cluster's Ambari UI for an overview of your cluster.</p>
<p>For further information please visit our <a href="http://docs.cloudbreak.apiary.io/#clusters">API documentation</a>.</p>
<!--howitworks.md-->

<!--technologies.md-->

<h2 id="technology">Technology</h2>
<p>Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Serf and dnsmasq.</p>
<h3 id="apache-ambari">Apache Ambari</h3>
<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>
<p><img alt="" src="img/ambari-overview.png?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL21hc3Rlci9kb2NzL2ltYWdlcy9hbWJhcmktb3ZlcnZpZXcucG5nIiwiZXhwaXJlcyI6MTQwNTQyNTQ0NH0%3D--b5c7b0e5adc62ebd34c8a5b73806f6ac905faed2" /></p>
<p>Ambari enables System Administrators to:</p>
<ol>
<li>Provision a Hadoop Cluster</li>
<li>Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li>Ambari handles configuration of Hadoop services for the cluster.</li>
<li>Manage a Hadoop Cluster</li>
<li>Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</li>
<li>Monitor a Hadoop Cluster</li>
<li>Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>Ambari leverages Ganglia for metrics collection.</li>
<li>Ambari leverages Nagios for system alerting and will send emails when your attention is needed (e.g., a node goes down, remaining disk space is low, etc).</li>
</ol>
<p>Ambari enables to integrate Hadoop provisioning, management, and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialize a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>
<p><img alt="" src="img/ambari-create-cluster.png?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL21hc3Rlci9kb2NzL2ltYWdlcy9hbWJhcmktY3JlYXRlLWNsdXN0ZXIucG5nIiwiZXhwaXJlcyI6MTQwNTQyNTUxOH0%3D--4207ee222d43a67deeb6e5174029cef45ea4f271" /></p>
<h3 id="docker">Docker</h3>
<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>
<p>The main features of Docker are:</p>
<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM - without the overhead of a VM</li>
<li>Each virtualized application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li>
<p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.
    <img alt="" src="img/vm.png?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL21hc3Rlci9kb2NzL2ltYWdlcy92bS5wbmciLCJleHBpcmVzIjoxNDA1NDI1NTc1fQ%3D%3D--b07a90b876bd4e26f361f37eed901f874f71bf87" /></p>
</li>
<li>
<p>Containers are isolated</p>
</li>
<li>It can be automated and scripted</li>
</ol>
<h3 id="serf">Serf</h3>
<p>Serf is a tool for cluster membership, failure detection, and orchestration that is decentralized, fault-tolerant and highly available. Serf runs on every major platform: Linux, Mac OS X, and Windows. It is extremely lightweight.
Serf uses an efficient gossip protocol to solve three major problems:</p>
<ul>
<li>
<p>Membership: Serf maintains cluster membership lists and is able to execute custom handler scripts when that membership changes. For example, Serf can maintain the list of Hadoop servers of a cluster and notify the members when nodes comes online or goes offline.</p>
</li>
<li>
<p>Failure detection and recovery: Serf automatically detects failed nodes within seconds, notifies the rest of the cluster, and executes handler scripts allowing you to handle these events. Serf will attempt to recover failed nodes by reconnecting to them periodically.
    <img alt="" src="img/serf-gossip.png?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL21hc3Rlci9kb2NzL2ltYWdlcy9zZXJmLWdvc3NpcC5wbmciLCJleHBpcmVzIjoxNDA1NDI1NjA4fQ%3D%3D--358fedd73c8fbd20a105dcc74bdf66524d680134" /></p>
</li>
<li>Custom event propagation: Serf can broadcast custom events and queries to the cluster. These can be used to trigger deploys, propagate configuration, etc. Events are simply fire-and-forget broadcast, and Serf makes a best effort to deliver messages in the face of offline nodes or network partitions. Queries provide a simple realtime request/response mechanism.
    <img alt="" src="img/serf-event.png?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL21hc3Rlci9kb2NzL2ltYWdlcy9zZXJmLWV2ZW50LnBuZyIsImV4cGlyZXMiOjE0MDU0MjU2Mjh9--69ea14f7dc1db34dd1dd6585df42ad8f2dd2a9d1" /></li>
</ul>
<!--technologies.md-->

<!--components.md-->

<h2 id="supported-components">Supported components</h2>
<p>Ambari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.</p>
<p>At high level the supported list of components can be grouped in to main categories: Master and Slave - and bundling them together form a Hadoop Service.</p>
<table>
<thead>
<tr>
<th align="left">Services</th>
<th align="left">Components</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">HDFS</td>
<td align="left">DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC</td>
</tr>
<tr>
<td align="left">YARN</td>
<td align="left">APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT</td>
</tr>
<tr>
<td align="left">MAPREDUCE2</td>
<td align="left">HISTORYSERVER, MAPREDUCE2_CLIENT</td>
</tr>
<tr>
<td align="left">GANGLIA</td>
<td align="left">GANGLIA_MONITOR, GANGLIA_SERVER</td>
</tr>
<tr>
<td align="left">HBASE</td>
<td align="left">HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER</td>
</tr>
<tr>
<td align="left">HIVE</td>
<td align="left">HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER</td>
</tr>
<tr>
<td align="left">HCATALOG</td>
<td align="left">HCAT</td>
</tr>
<tr>
<td align="left">WEBHCAT</td>
<td align="left">WEBHCAT_SERVER</td>
</tr>
<tr>
<td align="left">NAGIOS</td>
<td align="left">NAGIOS_SERVER</td>
</tr>
<tr>
<td align="left">OOZIE</td>
<td align="left">OOZIE_CLIENT, OOZIE_SERVER</td>
</tr>
<tr>
<td align="left">PIG</td>
<td align="left">PIG</td>
</tr>
<tr>
<td align="left">SQOOP</td>
<td align="left">SQOOP</td>
</tr>
<tr>
<td align="left">STORM</td>
<td align="left">DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR</td>
</tr>
<tr>
<td align="left">TEZ</td>
<td align="left">TEZ_CLIENT</td>
</tr>
<tr>
<td align="left">FALCON</td>
<td align="left">FALCON_CLIENT, FALCON_SERVER</td>
</tr>
<tr>
<td align="left">ZOOKEEPER</td>
<td align="left">ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER</td>
</tr>
</tbody>
</table>
<p>We provide a list of default Hadoop cluster Blueprints for your convenience, however you can always build and use your own Blueprint.</p>
<ol>
<li>Simple single node - Apache Ambari blueprint</li>
</ol>
<p>This is a simple <a href="https://raw.githubusercontent.com/sequenceiq/ambari-rest-client/master/src/main/resources/blueprints/single-node-hdfs-yarn">Blueprint</a> which allows you to launch a single node, pseudo-distributed Hadoop Cluster in the cloud.</p>
<p>It's allows you to use the following services: HDFS, YARN, MAPREDUCE2.</p>
<ol>
<li>Full stack single node - HDP 2.1 blueprint</li>
</ol>
<p>This is a complex <a href="https://raw.githubusercontent.com/sequenceiq/ambari-rest-client/master/src/main/resources/blueprints/hdp-singlenode-default">Blueprint</a> which allows you to launch a single node, pseudo-distributed Hadoop Cluster in the cloud.</p>
<p>It's allows you to use the following services: HDFS, YARN, MAPREDUCE2, GANGLIA, HBASE, HIVE, HCATALOG, WEBHCAT, NAGIOS, OOZIE, PIG, SQOOP, STORM, TEZ, FALCON, ZOOKEEPER.</p>
<ol>
<li>Simple multi node - Apache Ambari blueprint</li>
</ol>
<p>This is a simple <a href="https://raw.githubusercontent.com/sequenceiq/ambari-rest-client/master/src/main/resources/blueprints/multi-node-hdfs-yarn">Blueprint</a> which allows you to launch a multi node, fully distributed Hadoop Cluster in the cloud.</p>
<p>It's allows you to use the following services: HDFS, YARN, MAPREDUCE2.</p>
<ol>
<li>Full stack multi node - HDP 2.1 blueprint</li>
</ol>
<p>This is a complex <a href="https://raw.githubusercontent.com/sequenceiq/ambari-rest-client/master/src/main/resources/blueprints/hdp-multinode-default">Blueprint</a> which allows you to launch a multi node, fully distributed Hadoop Cluster in the cloud.</p>
<p>It's allows you to use the following services: HDFS, YARN, MAPREDUCE2, GANGLIA, HBASE, HIVE, HCATALOG, WEBHCAT, NAGIOS, OOZIE, PIG, SQOOP, STORM, TEZ, FALCON, ZOOKEEPER.</p>
<ol>
<li>Custom blueprints</li>
</ol>
<p>We allow you to build your own Blueprint - for further instractions please check the Apache Ambari <a href="https://cwiki.apache.org/confluence/display/AMBARI/Blueprints">documentation</a>.</p>
<p>When you are creating clustom Blueprints you can use the components above to build Hadoop services and use them in your on-demand Hadoop cluster.</p>
<p>We are trying to figure out the hosts to hostgroups assignements - and order to do so you will need to follow the conventions below:</p>
<p><em>Note: Apache Ambari community and SequenceIQ is working on an auto-hostgroup assignement algorithm; in the meantime please follow our conventions and check the default blueprints as examples, or ask us to support you.</em></p>
<ol>
<li>When you are creating a Single node blueprint the name of the default host group has to be <code>master</code>.</li>
<li>When yoy are creating a Multi node blueprint, all the worker node components (a.k.a. Slaves) will have to be grouped in host groups named <code>slave_*</code>. <em>Replace * with the number of Slave hostgroups</em>.</li>
</ol>
<p>The default rules are that for multi node clusters are there must be at least as many hosts as the number of host groups. Each NOT slave host groups (master, gateway, etc) will be launched wiht a cardinality of 1 (1 node per master, gateway, etc hosts), and all the rest of the nodes are equally distributed among Slave nodes (if there are multiple slave host groups).</p>
<!--components.md-->

<!--accounts.md-->

<h2 id="accounts">Accounts</h2>
<h3 id="cloudbreak-account">Cloudbreak account</h3>
<p>First and foremost in order to start launching Hadoop clusters you will need to create a Cloudbreak account. C
loudbreak supports registration, forgotten and reset password, and login features at API level.
All password are stored or sent are hashed - communication is always over a secure HTTPS channel. When you are deploying your own Cloudbreak instance we strongy suggest to configure an SSL certificate.
Users create and launch Hadoop clusters on their own namespace and security context.</p>
<p>Cloudbreak is launching Hadoop clusters on the user's behalf - on different cloud providers. One key point is that Cloudbreak <strong>does not</strong> store your Cloud provider account details (such as username, passord, keys, private SSL certificates, etc).
We work around the concept that Identity and Access Management is fully controlled by you - the end user. The Cloudbreak <em>deployer</em> is purely acting on behalf of the end user - without having access to the user's account.</p>
<p><strong>How does this work</strong>?</p>
<h3 id="configuring-the-aws-ec2-account">Configuring the AWS EC2 account</h3>
<p>Once you have logged in Cloudbreak you will have to link your AWS account with the Cloudbreak one. In order to do that you will need to configure an IAM Role.</p>
<ol>
<li>Log in to the AWS management console with the user account you'd like to use with Cloudbreak</li>
<li>Go to IAM and select Roles</li>
<li>
<p>Select Role for Cross-Account access</p>
<ul>
<li>Allows IAM users from a 3rd party AWS account to access this account.</li>
</ul>
<p><strong>Account ID:</strong> In case you are using our hosted solution you will need to pass SequenceIQ's account id: 755047402263</p>
<p><strong>External ID:</strong> provision-ambari (association link)</p>
<ul>
<li>Custom policy
  Use this policy <a href="https://raw.githubusercontent.com/sequenceiq/cloudbreak/documentation/src/main/resources/iam-arn-custom.policy?token=6003104__eyJzY29wZSI6IlJhd0Jsb2I6c2VxdWVuY2VpcS9jbG91ZGJyZWFrL2RvY3VtZW50YXRpb24vc3JjL21haW4vcmVzb3VyY2VzL2lhbS1hcm4tY3VzdG9tLnBvbGljeSIsImV4cGlyZXMiOjE0MDUzMzc2MjV9--cde4acae8c67317e6245598526be8cf680a08914">document</a> to configure the permission to start EC2 instances on the end user's behalf, and use SNS to receive notifications.</li>
</ul>
</li>
</ol>
<p>Once this is configured, Cloudbreak is ready to launch Hadoop clusters on your behalf. The only thing Cloudbreak requires is the <code>Role ARN</code> (Role for Cross-Account access).</p>
<h3 id="configuring-the-microsoft-azure-account">Configuring the Microsoft Azure account</h3>
<p>Once you have logged in Cloudbreak you will have to link your Azure account with the Cloudbreak one. Cloudbreak asks for your Azure <code>Subscription Id</code>, and will generate a <code>JKS</code> file and a <code>certificate</code> for you with your configured <code>passphrase</code>.
The JKS file and certificate (uploaded) will be used to encrypt the communication between Cloudbreak and Azure in both directions.
Additionally when you are creating Templates you can specify a <code>password</code> or an <code>SSH public key</code> in order for you to be able to login in the launched instances. As you can see the communication is bith directions is secure, and we will not be able to login into your instances.</p>
<p>In order to create a Cloudbreak account associated with your Azure account you will need to do the following steps.</p>
<ol>
<li>Log into Azure management console with the user account you'd like to use with Cloudbreak</li>
<li>On Azure go to Settings and Subscriptions tab - Cloudbreak will need your <code>Subscription Id</code> to associate accounts</li>
<li>On Cloudbreak API or UI you will have to add a password - this will be used as the <code>passphrase</code> for the JKS file and certificate we generate.</li>
<li>You should download the generated <code>certification</code></li>
<li>On Azure go to Settings and <code>Management Certificates</code> and upload the <code>cert</code> file</li>
</ol>
<p>You are done - from now on Cloudbreak can launch Azure instances on your behalf.</p>
<p><em>Note: Clodbreak does not store any login details into these instances - when you are creating Templates you can specify a <code>password</code> or <code>SSH Public key</code> which you can used to login into the launched instances.</em></p>
<!--accounts.md-->

<!--ui.md-->

<h2 id="cloudbreak-ui">Cloudbreak UI</h2>
<p>When we have started to work on Cloudbreak our main goal was to create an easy to use, cloud and Hadoop distribution agnostis Hadoop as a Service API. Though we always like to automate everything and aproach things with a very DevOps mindset, as a side project we have created a UI for Cloudbreak as well.
The goal of the UI is to ease to process and allow you to create a Hadoop cluster on your favorite cloud provider in <code>one-click</code>.</p>
<p>The UI is built on the foundation of the Cloudbreak REST API. <img alt="" src="img/UI-screenshot.png" /></p>
<h3 id="manage-credentials">Manage credentials</h3>
<p>Using manage credentials you can link your cloud account with the Cloudbreak account.</p>
<p><strong>Amazon AWS</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>Role ARN:</code> the role string - see Accounts</p>
<p><strong>Azure</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>Subscription Id:</code> your Azure subscription id - see Accounts</p>
<p><code>File password:</code> your generated JKS file password - see Accounts</p>
<h3 id="manage-templates">Manage templates</h3>
<p>Using manage templates you can create infrastructure templates.</p>
<p><strong>Amazon AWS</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>AMI:</code> the AMI which contains the Docker containers</p>
<p><code>SSH location:</code> allowed inbound SSH access. Use 0.0.0.0/0 as default</p>
<p><code>SSH key name:</code> your SSH key name used on AWS</p>
<p><code>Region:</code> AWS region where you'd like to launch your cluster</p>
<p><code>Instance type:</code> the Amazon instance type to be used - we suggest to use at least small or medium instances</p>
<p><strong>Azure</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>Location:</code> Azure datacenter location where you'd like to launch your cluster</p>
<p><code>Image name:</code> The Azure base image used</p>
<p><code>Instance type:</code> the Azure instance type to be used - we suggest to use at least small or medium instances</p>
<p><code>Password:</code> if you specify a password you can use it later to log into you launched instances</p>
<p><code>SSH public key</code> if you specify an SSH public key you can use your private key later to log into you launched instances</p>
<h3 id="manage-blueprints">Manage blueprints</h3>
<p>Blueprints are your declarative definition of a Hadoop cluster.</p>
<p><code>Name:</code> name of your blueprint</p>
<p><code>Description:</code> short description of your blueprint</p>
<p><code>Source URL:</code> you can add a blueprint by pointing to a URL. As an example you can use this <a href="https://raw.githubusercontent.com/sequenceiq/ambari-rest-client/master/src/main/resources/blueprints/multi-node-hdfs-yarn">blueprint</a>.</p>
<p><code>Manual copy:</code> you can copy paste your blueprint in this text area</p>
<p><em>Note: Apache Ambari community and SequenceIQ is working on an auto-hostgroup assignement algorithm; in the meantime please follow our conventions and check the default blueprints as examples, or ask us to support you.</em></p>
<p><em>1. When you are creating a Single node blueprint the name of the default host group has to be <code>master</code>.</em>
<em>2. When yoy are creating a Multi node blueprint, all the worker node components (a.k.a. Slaves) will have to be grouped in host groups named <code>slave_*</code>. Replace * with the number of Slave hostgroups.</em></p>
<p><em>The default rules are that for multi node clusters are there must be at least as many hosts as the number of host groups. Each NOT slave host groups (master, gateway, etc) will be launched wiht a cardinality of 1 (1 node per master, gateway, etc hosts), and all the rest of the nodes are equally distributed among Slave nodes (if there are multiple slave host groups).</em></p>
<h3 id="create-cluster">Create cluster</h3>
<p>Using the create cluster functionality you will create a cloud Stack and a Hadoop Cluster. In order to create a cluster you will have to select a credential first.
<em>Note: Cloudbreak can maintain multiple cloud credentials (even for the same provider).</em></p>
<p><code>Cluster name:</code> your cluster name</p>
<p><code>Cluster size:</code> the number of nodes in your Hadoop cluster</p>
<p><code>Template:</code> your cloud infrastructure template to be used</p>
<p><code>Blueprint:</code> your Hadoop cluster blueprint</p>
<p>Once you have launched the cluster creation you can track the progress either on Cloudbreak UI or your cloud provider management UI.</p>
<!--ui.md-->

<!--addnewcloud.md-->

<h2 id="add-new-cloud-providers">Add new cloud providers</h2>
<p>Cloudbreak is built from ground up on the idea of being cloud provider agnostic. All the external API's are cloud agnostic, and we have
internally abstracted wokring with individual cloud providers API's. Nevertheless adding new cloud providers is extremely important for us, thus
in order to speed up the process and linking the new provider API with Cloudbreak we came up with an SDK and list of responsibilities.
Once these interfaces are implemented, and the different providers API calls are <code>translated</code> you are ready to go.</p>
<p>Though we are working on a few popular providers to add to Cloudbreak we'd like to hear your voice as well - your ideas, provider requests or <code>contribution</code> is highly appreciated.</p>
<ol>
<li>
<p>Metadata service</p>
</li>
<li>
<p>Notifications</p>
</li>
<li>
<p>Account management</p>
</li>
</ol>
<!--addnewcloud.md-->

<!--releases.md-->

<h2 id="releases">Releases</h2>
<p>When we have started to work on Cloudbreak the idea was to <code>democratize</code> the usage of Hadoop in the cloud and VMs. For us this was a necesity as we often had to deal with different Hadoop versions, distributions and cloud providers.</p>
<p>Also we needed to find a way to speed up the process of adding new cloud providers, and be able to <code>ship</code> Hadoop between clouds without re-writing and re-engineering our code base each and every time - welcome <strong>Docker</strong>.</p>
<p>All the Hadoop ecosystem related code, configuration and serivces are inside Docker containers - and these containers are launched on VMs of cloud providers or physical hardware - the end result is the same: a <strong>resilient and dynamic</strong> Hadoop cluster.</p>
<p>We needed to find a unified way to provision, manage and configurure Hadoon clusters - welcome <strong>Apache Ambari</strong>.</p>
<h3 id="current-release-01">Current release - 0.1</h3>
<p>The first public beta version of Cloudbreak supports Hadoop on Amazon's EC2 and Microsoft's Azure cloud providers. The currently supported Hadoop is the Hortonworks Data Platform - the 100% open source Hadoop distribution.</p>
<p>Versions:</p>
<p>CentOS - 6.5
Hortonworks Data Platform - 2.1
Apache Hadoop - 2.4.0
Apache Tez - 0.4
Apache Pig - 0.12.1
Apache Hive &amp; HCatalog - 0.13.0
Apache HBase - 0.98.0
Apache Phoenix - 4.0.0
Apache Accumulo - 1.5.1
Apache Storm - 0.9.1
Apache Mahout - 0.9.0
Apache Solr - 4.7.2
Apache Falcon - 0.5.0
Apache Sqoop - 1.4.4
Apache Flume - 1.4.0
Apache Ambari - 1.6.1
Apache Oozie - 4.0.0
Apache Zookeeper - 3.4.5
Apache Knox - 0.4.0
Docker - 1.1
Serf - 0.5.0
dnsmasq - 2.7</p>
<h3 id="future-releases">Future releases</h3>
<p><strong>Hadoop distributions</strong></p>
<p>There is a great effort by the community and SequenceIQ to bring <a href="http://bigtop.apache.org/">Apache Bigtop</a> - the Apache Hadoop distribution - under the umbrella of Apache Ambari. Once this effort is finished, Cloudbreak will support Apache Bigtop as a Hadoop distribution as well.</p>
<p>In the meantime we have started an internal R&amp;D project to bring Cloudera's CDH distribution under Apache Ambari - in case you would like to collaborate in this task with us or sounds interesting to you don't hesitate to let us know.</p>
<p><strong>Cloud providers</strong></p>
<p>While we have just released the first public beta version of Cloudbreak, we have already started working on other cloud providers - namely Google Cloud Compute and Digital Ocean.
We have received many requests from people to integrate Cloudbreak wih 3d party hypervisors and cloud providers - as IBM's Softlayer. In case you'd like to have your favorite cloud provider listed don't hesitate to contact us or use our SDK and process to add yours.</p>
<p>Enjoy Cloudbreak - the Hadoop as a Service API which brings you a Hadoop ecosystem in matters on minutes. You are literarily one click or two REST calls away from a fully functional, distributed Hadoop cluster.</p>
<!--releases.md-->

</div>
        </div>

        

        <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
        <script src="./js/bootstrap-3.0.3.min.js"></script>
        <script src="./js/prettify-1.0.min.js"></script>
        <script src="./js/base.js"></script>
    </body>
</html>