<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="./img/favicon.ico">

        <title>Cloudbreak</title>
        <link href="./css/sequenceiq.doc.min.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href=".">Cloudbreak</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li class="disabled">
                    <a rel="next" >
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li class="disabled">
                    <a rel="prev" >
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#cloudbreak">Cloudbreak</a></li>
        
            <li><a href="#overview">Overview</a></li>
        
            <li><a href="#benefits">Benefits</a></li>
        
            <li><a href="#how-it-works">How it works?</a></li>
        
            <li><a href="#technology">Technology</a></li>
        
            <li><a href="#supported-components">Supported components</a></li>
        
            <li><a href="#accounts">Accounts</a></li>
        
            <li><a href="#roles">Roles</a></li>
        
            <li><a href="#cloudbreak-ui">Cloudbreak UI</a></li>
        
            <li><a href="#stack-lifecycle">Stack lifecycle</a></li>
        
            <li><a href="#quickstart-and-installation">QuickStart and installation</a></li>
        
            <li><a href="#releases-future-plans">Releases, future plans</a></li>
        
            <li><a href="#contribution">Contribution</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<!--main.md-->

<h1 id="cloudbreak">Cloudbreak</h1>
<p><em>Cloudbreak is a powerful left surf that breaks over a coral reef, a mile off southwest the island of Tavarua, Fiji.</em></p>
<p><em>Cloudbreak is a cloud agnostic Hadoop as a Service API. Abstracts the provisioning and ease management and monitoring of on-demand clusters.</em></p>
<p>Cloudbreak <a href="https://cloudbreak-api.sequenceiq.com/api/index.html">API documentation</a>.</p>
<!--main.md-->

<!--overview.md-->

<h2 id="overview">Overview</h2>
<p>Cloudbreak is a RESTful Hadoop as a Service API. Once it is deployed in your favorite servlet container exposes a REST API allowing to span up Hadoop clusters of arbitrary sizes on your selected cloud provider. Provisioning Hadoop has never been easier.
Cloudbreak is built on the foundation of cloud providers API (Microsoft Azure, Amazon AWS, Google Cloud Platform, OpenStack), Apache Ambari, Docker containers, Swarm and Consul.</p>
<h2 id="benefits">Benefits</h2>
<h3 id="secure">Secure</h3>
<p>Supports basic, token based and OAuth2 authentication model. The cluster is provisioned in a logically isolated network (Virtual Private Cloud) of your favorite cloud provider. Cloudbreak does not store or manage your cloud credentials - it is the end user's responsibility to link the Cloudbreak user with her/his cloud account.</p>
<h3 id="elastic">Elastic</h3>
<p>Using Cloudbreak API you can provision an arbitrary number of Hadoop nodes - the API does the hard work for you, and span up the infrastructure, configure the network and the selected Hadoop components and services without any user interaction. POST once and use it anytime after.</p>
<h3 id="scalable">Scalable</h3>
<p>As your workload changes, the API allows you to add or remove nodes on the fly. Cloudbreak does the hard work of reconfiguring the infrastructure, provision or decommission Hadoop nodes and let the cluster be continuously operational. Once provisioned, new nodes will take up the load and increase the cluster throughput.</p>
<h3 id="declarative-hadoop-clusters">Declarative Hadoop clusters</h3>
<p>We support declarative Hadoop cluster creation - using blueprints. Blueprints are a declarative definition of your stack, the component/services layout and the configurations to materialize a Hadoop cluster instance.</p>
<h3 id="flexible">Flexible</h3>
<p>You have the option to choose your favorite cloud provider and their different pricing models. The API translates the calls towards different vendors. Nevertheless you integrate and use one common API, there is no need to rewrite your code when changing between cloud providers.</p>
<!--overview.md-->

<!--howitworks.md-->

<h2 id="how-it-works">How it works?</h2>
<p>Cloudbreak launches on-demand Hadoop clusters on your favorite cloud provider in minutes. We have introduced 4 main notions - the core building blocks of the REST API.</p>
<h3 id="templates">Templates</h3>
<p>A template gives developers and systems administrators an easy way to create and manage a collection of cloud infrastructure related <code>resources</code>, maintaining and updating them in an orderly and predictable fashion.
Templates are cloud specific - and on top of the infrastructural setup they collect the information such as the used machine images, the datacenter location, instance types, and can capture and control region-specific infrastructure variations. We support heterogenous clusters - this means that one Hadoop cluster can be built by combining different templates.</p>
<p>A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a new stack).</p>
<p>The infrastructure specific configuration is available under the Cloudbreak <a href="https://github.com/sequenceiq/cloudbreak/blob/master/src/main/resources/templates">resources</a>.
As an example for Amazon EC2, we use <a href="http://aws.amazon.com/cloudformation/">AWS Cloudformation</a> to define the cloud infrastructure.</p>
<p>For further information please visit our <a href="https://cloudbreak-api.sequenceiq.com/api/index.html#/templates">API documentation</a>.</p>
<h3 id="stacks">Stacks</h3>
<p>Stacks are template <code>instances</code> - a running cloud infrastructure created based on a template. Stacks are always launched on behalf of a cloud user account. Stacks support a wide range of resources, allowing you to build a highly available, reliable, and scalable infrastructure for your application needs.</p>
<p>For further information please visit our <a href="https://cloudbreak-api.sequenceiq.com/api/index.html#/stack">API documentation</a>.</p>
<h3 id="blueprints">Blueprints</h3>
<p>Ambari Blueprints are a declarative definition of a Hadoop cluster. With a Blueprint, you specify a stack, the component layout and the configurations to materialize a Hadoop cluster instance. Hostgroups defined in blueprints can be associated to different templates, thus you can spin up a highly available cluster running on different instance types. This will give you the option to group your Hadoop services based on resource needs (e.g. high I/O, CPU or memory) and create an infrastructure which fits your workload best.</p>
<p>We have a few default blueprints available from multinode, streaming to analytic ones.</p>
<p>For further information please visit our <a href="https://cloudbreak-api.sequenceiq.com/api/index.html#/blueprints">API documentation</a>.</p>
<h3 id="cluster">Cluster</h3>
<p>Clusters are materialized Hadoop services on a given infrastructure. They are built based on a Blueprint (running the components and services specified) and on a configured infrastructure Stack.
Once a cluster is created and launched, it can be used the usual way as any Hadoop cluster. We suggest to start with the Cluster's Ambari UI for an overview of your cluster.</p>
<p>For further information please visit our <a href="https://cloudbreak-api.sequenceiq.com/api/index.html#/cluster">API documentation</a>.</p>
<!--howitworks.md-->

<!--technologies.md-->

<h2 id="technology">Technology</h2>
<p>Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Swarm and Consul.</p>
<h3 id="apache-ambari">Apache Ambari</h3>
<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-overview.png" /></p>
<p>Ambari enables System Administrators to:</p>
<ol>
<li>Provision a Hadoop Cluster</li>
<li>Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li>
<p>Ambari handles configuration of Hadoop services for the cluster.</p>
</li>
<li>
<p>Manage a Hadoop Cluster</p>
</li>
<li>
<p>Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</p>
</li>
<li>
<p>Monitor a Hadoop Cluster</p>
</li>
<li>Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>Ambari leverages Ganglia for metrics collection.</li>
<li>Ambari leverages Nagios for system alerting and will send emails when your attention is needed (e.g. a node goes down, remaining disk space is low, etc).</li>
</ol>
<p>Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialise a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/ambari-create-cluster.png" /></p>
<h3 id="docker">Docker</h3>
<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>
<p>The main features of Docker are:</p>
<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM - without the overhead of a VM</li>
<li>Each virtualised application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li>
<p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.
    <img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/vm.png" /></p>
</li>
<li>
<p>Containers are isolated</p>
</li>
<li>It can be automated and scripted</li>
</ol>
<h3 id="swarm">Swarm</h3>
<p>Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual host. Swarm serves the standard Docker API.</p>
<ul>
<li>Distributed container orchestration: Allows to remotely orchestrate Docker containers on different hosts
    <img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/swarm.png" /></li>
<li>Discovery services: Supports different discovery backends to provide service discovery, as such: token (hosted) and file based, etcd, Consul, Zookeeper.</li>
<li>Advanced scheduling: Swarm will schedule containers on hosts based on different filters and strategies</li>
</ul>
<h3 id="consul">Consul</h3>
<p>Consul it is a tool for discovering and configuring services in your infrastructure. It provides several key features</p>
<ul>
<li>
<p>Service Discovery: Clients of Consul can provide a service, such as api or mysql, and other clients can use Consul to discover providers of a given service. Using either DNS or HTTP, applications can easily find the services they depend upon.</p>
</li>
<li>
<p>Health Checking: Consul clients can provide any number of health checks, either associated with a given service ("is the webserver returning 200 OK"), or with the local node ("is memory utilization below 90%"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.</p>
</li>
<li>
<p>Key/Value Store: Applications can make use of Consul's hierarchical key/value store for any number of purposes, including dynamic configuration, feature flagging, coordination, leader election, and more. The simple HTTP API makes it easy to use.</p>
</li>
<li>
<p>Multi Datacenter: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/consul.png" /></p>
</li>
</ul>
<!--technologies.md-->

<!--components.md-->

<h2 id="supported-components">Supported components</h2>
<p>Ambari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.</p>
<p>At high level the supported list of components can be grouped into main categories: Master and Slave - and bundling them together form a Hadoop Service.</p>
<table>
<thead>
<tr>
<th align="left">Services</th>
<th align="left">Components</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">HDFS</td>
<td align="left">DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC</td>
</tr>
<tr>
<td align="left">YARN</td>
<td align="left">APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT</td>
</tr>
<tr>
<td align="left">MAPREDUCE2</td>
<td align="left">HISTORYSERVER, MAPREDUCE2_CLIENT</td>
</tr>
<tr>
<td align="left">GANGLIA</td>
<td align="left">GANGLIA_MONITOR, GANGLIA_SERVER</td>
</tr>
<tr>
<td align="left">HBASE</td>
<td align="left">HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER</td>
</tr>
<tr>
<td align="left">HIVE</td>
<td align="left">HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER</td>
</tr>
<tr>
<td align="left">HCATALOG</td>
<td align="left">HCAT</td>
</tr>
<tr>
<td align="left">WEBHCAT</td>
<td align="left">WEBHCAT_SERVER</td>
</tr>
<tr>
<td align="left">OOZIE</td>
<td align="left">OOZIE_CLIENT, OOZIE_SERVER</td>
</tr>
<tr>
<td align="left">PIG</td>
<td align="left">PIG</td>
</tr>
<tr>
<td align="left">SQOOP</td>
<td align="left">SQOOP</td>
</tr>
<tr>
<td align="left">STORM</td>
<td align="left">DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR</td>
</tr>
<tr>
<td align="left">TEZ</td>
<td align="left">TEZ_CLIENT</td>
</tr>
<tr>
<td align="left">FALCON</td>
<td align="left">FALCON_CLIENT, FALCON_SERVER</td>
</tr>
<tr>
<td align="left">ZOOKEEPER</td>
<td align="left">ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER</td>
</tr>
<tr>
<td align="left">SPARK</td>
<td align="left">SPARK_JOBHISTORYSERVER, SPARK_CLIENT</td>
</tr>
<tr>
<td align="left">RANGER</td>
<td align="left">RANGER_USERSYNC, RANGER_ADMIN</td>
</tr>
<tr>
<td align="left">AMBARI_METRICS</td>
<td align="left">AMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR</td>
</tr>
<tr>
<td align="left">KERBEROS</td>
<td align="left">KERBEROS_CLIENT</td>
</tr>
<tr>
<td align="left">FLUME</td>
<td align="left">FLUME_HANDLER</td>
</tr>
<tr>
<td align="left">KAFKA</td>
<td align="left">KAFKA_BROKER</td>
</tr>
<tr>
<td align="left">KNOX</td>
<td align="left">KNOX_GATEWAY</td>
</tr>
<tr>
<td align="left">NAGIOS</td>
<td align="left">NAGIOS_SERVER</td>
</tr>
</tbody>
</table>
<p>We provide a list of default Hadoop cluster Blueprints for your convenience, however you can always build and use your own Blueprint.</p>
<ol>
<li>hdp-small-default - HDP 2.2 blueprint</li>
</ol>
<p>This is a complex <a href="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/core/src/main/resources/defaults/blueprints/hdp-small-default.bp">Blueprint</a> which allows you to launch a multi node, fully distributed HDP 2.2 Cluster in the cloud.</p>
<p>It allows you to use the following services: HDFS, YARN, MAPREDUCE2, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.</p>
<ol>
<li>hdp-streaming-cluster - HDP 2.2 blueprint</li>
</ol>
<p>This is a streaming <a href="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/core/src/main/resources/defaults/blueprints/hdp-streaming-cluster.bp">Blueprint</a> which allows you to launch a multi node, fully distributed HDP 2.2 Cluster in the cloud, optimized for streaming jobs.</p>
<p>It allows you to use the following services: HDFS, YARN, MAPREDUCE2, STORM, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.</p>
<ol>
<li>hdp-spark-cluster - HDP 2.2 blueprint</li>
</ol>
<p>This is an analytics [Blueprint]https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/core/src/main/resources/defaults/blueprints/hdp-spark-cluster.bp) which allows you to launch a multi node, fully distributed HDP 2.2 Cluster in the cloud, optimized for analytic jobs.</p>
<p>It allows you to use the following services: HDFS, YARN, MAPREDUCE2, SPARK, ZEPPELIN, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER.</p>
<!--components.md-->

<!--accounts.md-->

<h2 id="accounts">Accounts</h2>
<h3 id="cloudbreak-account">Cloudbreak account</h3>
<p>First and foremost in order to start launching Hadoop clusters you will need to create a Cloudbreak account.
Cloudbreak supports registration, forgotten and reset password, and login features at API level.
All passwords that are stored or sent are hashed - communication is always over a secure HTTPS channel. When you are deploying your own Cloudbreak instance we strongly suggest to configure an SSL certificate. Users create and launch Hadoop clusters on their own namespace and security context.</p>
<p>Users can be invited under an account by the administrator, and all resources (e.g. resources, networks, blueprints, credentials, clusters) can be shared across account users.</p>
<ul>
<li>
<p>Usage explorer: Cloudbreak gives you an up to date overview of cluster usage based on different filtering criteria (start/end date, users, providers, region, etc).</p>
</li>
<li>
<p>Account details: The account details of the user.</p>
</li>
<li>
<p>Manage users: You can invite, active and deactivate users under the account.</p>
</li>
</ul>
<h3 id="cloudbreak-credentials">Cloudbreak credentials</h3>
<p>Cloudbreak is launching Hadoop clusters on the user's behalf - on different cloud providers. One key point is that Cloudbreak <strong>does not</strong> store your Cloud provider account details (such as username, password, keys, private SSL certificates, etc).
We work around the concept that Identity and Access Management is fully controlled by you - the end user. The Cloudbreak <em>deployer</em> is purely acting on behalf of the end user - without having access to the user's account.</p>
<p><strong>How does this work</strong>?</p>
<h3 id="configuring-the-aws-ec2-account">Configuring the AWS EC2 account</h3>
<p>Once you have logged in Cloudbreak you will have to link your AWS account with the Cloudbreak one. In order to do that you will need to configure an IAM Role.
You can do this on the management console, or - if you have aws-cli configured - by running a small script we're providing in the <code>docs/aws</code> folder.</p>
<h4 id="create-iam-role-on-the-console">Create IAM role on the console</h4>
<ol>
<li>Log in to the AWS management console with the user account you'd like to use with Cloudbreak</li>
<li>Go to IAM, select "Roles", click Create New Role</li>
<li>Give your role a name.</li>
<li>Setup your role access:</li>
<li>
<p>Select Role for Cross-Account access</p>
<ul>
<li>Allows IAM users from a 3rd party AWS account to access this account.</li>
</ul>
<p><strong>Account ID:</strong> In case you are using our hosted solution you will need to pass SequenceIQ's account id: 755047402263</p>
<p><strong>External ID:</strong> provision-ambari (association link)</p>
<ul>
<li>Custom policy</li>
</ul>
<p>Use this policy <strong><a href="https://s3-eu-west-1.amazonaws.com/sequenceiq/iam_role.policy">document</a></strong> to configure the permission to start EC2 instances on the end user's behalf, and use SNS to receive notifications.</p>
</li>
</ol>
<h4 id="create-iam-role-with-the-create-script">Create IAM role with the create script</h4>
<ol>
<li>Download <a href="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/aws/create-iam-role.sh">this script</a>, e.g: <code>curl -O https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/aws/create-iam-role.sh</code></li>
<li>Make sure you have the <a href="http://aws.amazon.com/cli/">AWS CLI</a> installed and on your path.</li>
<li>Run <code>./create-iam-role</code></li>
<li>Copy the resulting role ARN</li>
</ol>
<p>Once this is configured, Cloudbreak is ready to launch Hadoop clusters on your behalf. The only thing Cloudbreak requires is the <code>Role ARN</code> (Role for Cross-Account access).</p>
<h3 id="configuring-the-microsoft-azure-account">Configuring the Microsoft Azure account</h3>
<p>In order to launch Hadoop clusters on the  Microsoft Azure cloud platform you'll need to link your Azure account with Cloudbreak. This can be achieved by creating a new <code>Azure Credential</code> in Cloudbreak.</p>
<p>You'll need an X509 certificate with a 2048-bit RSA keypair.</p>
<p>Generate these artifacts with <code>openssl</code> by running the following command, and answering the questions in the command prompt:</p>
<pre class="prettyprint well"><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout my_azure_private.key -out my_azure_cert.pem
</code></pre>

<p>The command generates the following files into the directory you run the command from: <code>my_azure_private.key</code> and <code>my_azure_cert.pem</code></p>
<p>Fill the form by providing your Azure <code>Subscription Id</code>, and the <strong>content</strong> of the previously generated certificate (my_azure_cert.pem).</p>
<p><em>Note:</em> Cloudbreak will generate a <code>JKS</code> file (stored by the backend) and a <code>certificate</code> with the <code>passphrase</code>. You will need to upload the generated certificate (that is automatically downloaded to you after the form submission, alternatively you can download it any time from Cloudbreak) to your Azure account:</p>
<p>On your <code>Azure Management Console</code>, <code>Settings</code> menu, click the <code>Management Certificates</code> tab and upload the downloaded <code>cert</code> file</p>
<p>The JKS file and certificate will be used to encrypt the communication between Cloudbreak and Azure in both directions.</p>
<p>You are done - from now on Cloudbreak can launch Azure instances on your behalf.</p>
<p><em>Note:</em> Cloudbreak does not store any login details for these instances - you can specify a <code>password</code> or <code>SSH Public key</code> that can be used to login into the launched instances.</p>
<p><em>Note:</em> Because Azure does not directly support third party public images Cloudbreak will copy the used image from VM Depot into your storage account. The steps below need to be finished ONCE and only ONCE before any stack is created for every affinity group - <strong>this is an automated step</strong>  - it's only highlighted here as an explanation of why provisioning takes longer at first time:</p>
<p><em>1. Get the VM image - http://vmdepot.msopentech.com/Vhd/Show?vhdId=42480&amp;version=43564</em></p>
<p><em>2. Copy the VHD blob from above (community images) into your storage account</em></p>
<p><em>3. Create a VM image from the copied VHD blob.</em></p>
<p><em>This process will take 20 minutes so be patient - but this step will have do be done once and only once.</em></p>
<h3 id="configuring-the-google-cloud-account">Configuring the Google Cloud account</h3>
<p>Follow the <a href="https://cloud.google.com/storage/docs/authentication#service_accounts">instructions</a> in Google Cloud's documentation to create a <code>Service account</code> and <code>Generate a new P12 key</code>.</p>
<p>Make sure that at API level (<strong>APIs and auth</strong> menu) you have enabled:</p>
<ul>
<li>Google Compute Engine</li>
<li>Google Compute Engine Instance Group Manager API</li>
<li>Google Compute Engine Instance Groups API</li>
<li>BigQuery API</li>
<li>Google Cloud Deployment Manager API</li>
<li>Google Cloud DNS API</li>
<li>Google Cloud SQL</li>
<li>Google Cloud Storage</li>
<li>Google Cloud Storage JSON API</li>
</ul>
<p>When creating GCP credentials in Cloudbreak you will have to provide the email address of the Service Account and the project ID (from Google Developers Console - Projects) where the service account is created. You'll also have to upload the generated P12 file and provide an OpenSSH formatted public key that will be used as an SSH key.</p>
<!--accounts.md-->

<h2 id="roles">Roles</h2>
<p>Cloudbreak defines three distinct roles:</p>
<ol>
<li>DEPLOYER</li>
<li>ACCOUNT_ADMIN</li>
<li>ACCOUNT_USER</li>
</ol>
<h3 id="cloudbreak-deployer">Cloudbreak deployer</h3>
<p>This is the <code>master</code> role - the user which is created during the deployment process will have this role.</p>
<h3 id="account-admin">Account admin</h3>
<p>We have introduced the notion of accounts - and with that comes an administrator role. Upon registration a user will become an account administrator.</p>
<p>The extra rights associated with the account admin role are:</p>
<ul>
<li>Invite users to join the account</li>
<li>Share account wide resources (credential, blueprints, templates)</li>
<li>See resources created by account users</li>
<li>Monitor clusters started by account users</li>
<li>Management and reporting tool available</li>
</ul>
<h3 id="account-user">Account user</h3>
<p>An account user is a user who has been invited to join Cloudbreak by an account administrator. Account users activity will show up in the management and reporting tool for account wide statistics - accessible by the account administrator. Apart from common account wide resources, the account users can manage their own private resources.</p>
<!--ui.md-->

<h2 id="cloudbreak-ui">Cloudbreak UI</h2>
<p>When we have started to work on Cloudbreak, our main goal was to create an easy to use, cloud and Hadoop distribution agnostic Hadoop as a Service API. Though we always like to automate everything and approach things with a very DevOps mindset, as a side project we have created a UI for Cloudbreak as well.
The goal of the UI is to ease to process and allow you to create a Hadoop cluster on your favorite cloud provider in <code>one-click</code>.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/UI-screenshot.png" /></p>
<p>The UI is built on the foundation of the Cloudbreak REST API. You can access the UI <a href="https://cloudbreak.sequenceiq.com/">here</a>.</p>
<h3 id="user-registration">User registration</h3>
<p>While we consider the registration process quite simple, we'd like to explain the notion of <strong>companies</strong>. When a user registers as a <code>company admin</code> it means that he will be the administrator of that company - further colleagues will have the opportunity to join the <strong>company</strong> upon being invited by the admin user.</p>
<h3 id="manage-credentials">Manage credentials</h3>
<p>Using manage credentials you can link your cloud account with the Cloudbreak account.</p>
<p><strong>Amazon AWS</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>Role ARN:</code> the role string - you can find it at the summary tab of the IAM role</p>
<p><code>SSH public key:</code> an SSH public key in OpenSSH format that's private keypair can be used to log into the launched instances later</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p>The ssh username is ec2-user</p>
<p><strong>Azure</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>Subscription Id:</code> your Azure subscription id - see Accounts</p>
<p><code>File password:</code> your generated JKS file password - see Accounts</p>
<p><code>SSH certificate:</code> the SSH public certificate in OpenSSH format that's private keypair can be used to log into the launched instances later (The key generation process is described in the Configuring the Microsoft Azure account section)</p>
<p>The ssh username is cloudbreak</p>
<p><strong>Google Cloud Platform</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>Project Id:</code> your GCP Project id - see Accounts</p>
<p><code>Service Account Email Address:</code> your GCP service account mail address - see Accounts</p>
<p><code>Service Account private (p12) key:</code> your GCP service account generated private key - see Accounts</p>
<p><code>SSH public key:</code> the SSH public key in OpenSSH format that's private keypair can be used to log into the launched instances later</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p>The ssh username is cloudbreak.</p>
<p><strong>OpenStack</strong></p>
<p><code>Name:</code> name of your credential</p>
<p><code>Description:</code> short description of your linked credential</p>
<p><code>User:</code> OpenStack user name</p>
<p><code>Password:</code> OpenStack user's password name</p>
<p><code>Tenant Name:</code> OpenStack tenant's (project) name</p>
<p><code>Endpoint:</code> OpenStack API address endpoint's</p>
<p><code>SSH public key:</code> the SSH public key to be used to log into the launched instances later</p>
<p><code>Public in account:</code> share it with others in the account</p>
<h3 id="manage-resources">Manage resources</h3>
<p>Using manage resources you can create infrastructure templates.</p>
<p><strong>Amazon AWS</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>Instance type:</code> the Amazon instance type to be used - we suggest to use at least small or medium instances</p>
<p><code>Volume type:</code> option to choose are SSD, regular HDD (both EBS) or Ephemeral</p>
<p><code>Attached volumes per instance:</code> the number of disks to be attached</p>
<p><code>Volume size (GB):</code> the size of the attached disks (in GB)</p>
<p><code>Spot price:</code> option to set a spot price - not mandatory, if specified we will request spot price instances (which might take a while or never be fulfilled by Amazon)</p>
<p><code>EBS encryption:</code> this feature is supported with all EBS volume types (General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p><strong>Azure</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>Instance type:</code> the Azure instance type to be used - we suggest to use at least D2 or D4 instances</p>
<p><code>Attached volumes per instance:</code> the number of disks to be attached</p>
<p><code>Volume size (GB):</code> the size of the attached disks (in GB)</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p><strong>Google Cloud Platform</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>Instance type:</code> the Google instance type to be used - we suggest to use at least small or medium instances</p>
<p><code>Volume type:</code> option to choose SSD or regular HDD</p>
<p><code>Attached volumes per instance:</code> the number of disks to be attached</p>
<p><code>Volume size (GB):</code> the size of the attached disks (in GB)</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p><strong>OpenStack</strong></p>
<p><code>Name:</code> name of your template</p>
<p><code>Description:</code> short description of your template</p>
<p><code>Public Net Id:</code> the public net id</p>
<p><code>Attached volumes per instance:</code> the number of disks to be attached</p>
<p><code>Volume size (GB):</code> the size of the attached disks (in GB)</p>
<p><code>Public in account:</code> share it with others in the account</p>
<h3 id="manage-blueprints">Manage blueprints</h3>
<p>Blueprints are your declarative definition of a Hadoop cluster.</p>
<p><code>Name:</code> name of your blueprint</p>
<p><code>Description:</code> short description of your blueprint</p>
<p><code>Source URL:</code> you can add a blueprint by pointing to a URL. As an example you can use this <a href="https://github.com/sequenceiq/ambari-rest-client/raw/1.6.0/src/main/resources/blueprints/multi-node-hdfs-yarn">blueprint</a>.</p>
<p><code>Manual copy:</code> you can copy paste your blueprint in this text area</p>
<p><code>Public in account:</code> share it with others in the account</p>
<h3 id="manage-networks">Manage networks</h3>
<p>Manage networks allows you to create or reuse existing networks and configure them.</p>
<p><code>Name:</code> name of the network</p>
<p><code>Description:</code> short description of your network</p>
<p><code>Subnet (CIDR):</code> a subnet in the VPC with CIDR block</p>
<p><code>Address prefix (CIDR):</code> the address space that is used for subnets (Azure only)</p>
<p><code>Public network ID:</code> the publuc network id (OpenStack only)</p>
<p><code>Public in account:</code> share it with others in the account</p>
<h3 id="create-cluster">Create cluster</h3>
<p>Using the create cluster functionality you will create a cloud Stack and a Hadoop Cluster. In order to create a cluster you will have to select a credential first.
<em>Note: Cloudbreak can maintain multiple cloud credentials (even for the same provider).</em></p>
<p><code>Cluster name:</code> your cluster name</p>
<p><code>Region</code>: the region where the cluster is started</p>
<p><code>Blueprint:</code> your Hadoop cluster blueprint</p>
<p>Once the blueprint is selected we parse it and give you the option to select the followings for each <strong>hostgroup</strong>.</p>
<p><code>Group size:</code> the number of instances to be started</p>
<p><code>Template:</code> the stack template associated to the hostgroup</p>
<p><code>Public in account:</code> share it with others in the account</p>
<p>Once you have launched the cluster creation you can track the progress either on Cloudbreak UI or your cloud provider management UI.</p>
<p><em>Note: Because Azure does not directly support third party public images we will have to copy the used image from VM Depot into your storage account. The steps below need to be finished once and only once before any stack is created for every affinity group:</em></p>
<p><em>1. Get the VM image - http://vmdepot.msopentech.com/Vhd/Show?vhdId=42480&amp;version=43564</em></p>
<p><em>2. Copy the VHD blob from above (community images) into your storage account</em></p>
<p><em>3. Create a VM image from the copied VHD blob.</em></p>
<p><em>This process will take 20 minutes so be patient - but this step will have do be done once and only once.</em></p>
<!--ui.md-->

<!--states.md-->

<h2 id="stack-lifecycle">Stack lifecycle</h2>
<p>In order to understand the state of your Hadoop as a Servie stack and the potential outcomes we have put together a UML state diagram.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/stack_state_diag.png" /></p>
<!--states.md-->

<!--quickstart.md-->

<h2 id="quickstart-and-installation">QuickStart and installation</h2>
<p>We provide you two different ways to start using Cloudbreak. The simplest and easiest solution is hosted by SequenceIQ, however we encourage you to get Cloudbreak and deployed it on-premise or on your favorite cloud provider. The hosted solution is for demo purposes only and not recommended for production systems.</p>
<h3 id="hosted-cloudbreak-ui-and-api">Hosted - Cloudbreak UI and API</h3>
<p>The easiest way to start your own Hadoop cluster in your favorite cloud provider is to use our hosted solution. We host, maintain and support <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> for you.</p>
<p>Please note that Cloudbreak is launching Hadoop clusters on the user's behalf - on different cloud providers. We do not store your cloud provider account details (such as username, password, keys, private SSL certificates, etc), but work around the concept that Identity and Access Management is fully controlled by you - the end user.</p>
<p>Though Cloudbreak controls your Hadoop cluster lifecycle (start, stop, pause), we <strong>do not</strong> have access to the launched instances. The Hadoop clusters created by Cloudbreak are private to you.</p>
<h3 id="diy-deploying-cloudbreak-ui-and-api">DIY - Deploying Cloudbreak UI and API</h3>
<p>Cloudbreak is deployed as orchestrated Docker containers. We created a project which guides you through self deployment:
https://github.com/sequenceiq/cloudbreak-deployer</p>
<!--quickstart.md-->

<!--releases.md-->

<h2 id="releases-future-plans">Releases, future plans</h2>
<p>When we have started to work on Cloudbreak the idea was to <code>democratise</code> the usage of Hadoop in the cloud and VMs. For us this was a necessity as we often had to deal with different Hadoop versions, distributions and cloud providers.</p>
<p>Also we needed to find a way to speed up the process of adding new cloud providers, and be able to <code>ship</code> Hadoop between clouds without re-writing and re-engineering our code base each and every time - welcome <strong>Docker</strong>.</p>
<p>All the Hadoop ecosystem related code, configuration and services are inside Docker containers - and these containers are launched on VMs of cloud providers or physical hardware - the end result is the same: a <strong>resilient and dynamic</strong> Hadoop cluster.</p>
<p>We needed to find a unified way to provision, manage and configure Hadoop clusters - welcome <strong>Apache Ambari</strong>.</p>
<h3 id="public-beta">Public Beta</h3>
<p>The first public beta version of Cloudbreak supports Hadoop on Amazon's EC2, Microsoft's Azure, Google Cloud Platform and OpenStack cloud providers. The supported Hadoop platform is the Hortonworks Data Platform - the 100% open source Hadoop distribution.</p>
<p>Versions:</p>
<p>CentOS - 6.5
Hortonworks Data Platform - 2.2
Apache Hadoop - 2.6.0
Apache Tez - 0.6
Apache Pig - 0.14
Apache Hive &amp; HCatalog - 0.14.0
Apache HBase - 0.98.4
Apache Phoenix - 4.2
Apache Accumulo - 1.6.1
Apache Storm - 0.9.3
Apache Spark - 1.2.0
Apache Slider - 0.5.1
Apache Solr -  4.10.0
Apache Kafka - 0.8.1
Apache Falcon - 0.6.0
Apache Sqoop - 1.4.5
Apache Flume - 1.5.0
Apache Ambari - 1.7.0
Apache Oozie - 4.1.0
Apache Zookeeper - 3.4.5
Apache Knox - 0.5.0
Docker - 1.6.2
Swarm - 0.2
Consul - 0.5</p>
<h3 id="future-releases">Future releases</h3>
<h4 id="hadoop-distributions">Hadoop distributions</h4>
<p>There is an effort by the community to bring <a href="http://bigtop.apache.org/">Apache Bigtop</a> - the Apache Hadoop distribution - under the umbrella of Ambari. Once this effort is finished, Cloudbreak will support Apache Bigtop as a Hadoop distribution as well.</p>
<p>Apache Ambari allows you to create your own <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=38571133">custom Hadoop stack</a> - and you can use Cloudbreak to provision a cluster based on that.</p>
<h4 id="cloud-providers">Cloud providers</h4>
<p>Supported cloud providers:</p>
<ul>
<li>Amazon AWS</li>
<li>Microsoft Azure</li>
<li>Google Cloud Platform</li>
<li>OpenStack</li>
</ul>
<p>While we have just released the first public beta version of Cloudbreak, we have already started working on other cloud providers - namely <em>Rackspace</em> and <em>HP Helion Public Cloud</em>.
We have received many requests from people to integrate Cloudbreak with 3d party hypervisors and cloud providers - as IBM's SoftLayer. In case you'd like to have your favorite cloud provider listed don't hesitate to contact us or use our SDK and process to add yours. You can fill the following <a href="https://docs.google.com/forms/d/129RVh6VfjRsuuHOcS3VPbFYTdM2SEjANDsGCR5Pul0I/viewform">questionnaire</a> and request your favorite cloud provider. In case you'd like to integrate your favorite provider we are happy to support you and merge your contribution.</p>
<p>Enjoy Cloudbreak - the Hadoop as a Service API which brings you a Hadoop ecosystem in minutes. You are literaly one click or REST call away from a fully functional, distributed Hadoop cluster.</p>
<!--releases.md-->

<h2 id="contribution">Contribution</h2>
<p>So you are about to contribute to Cloudbreak? Awesome! There are many different ways in which you can contribute. We strongly value your feedback, questions, bug reports, and feature requests.
Cloudbreak consist of the following main projects:</p>
<h3 id="cloudbreak-ui_1">Cloudbreak UI</h3>
<p>Available: <a href=https://cloudbreak.sequenceiq.com>https://cloudbreak.sequenceiq.com</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/uluwatu>https://github.com/sequenceiq/uluwatu</a></p>
<h3 id="cloudbreak-api">Cloudbreak API</h3>
<p>Available: <a href=https://cloudbreak-api.sequenceiq.com>https://cloudbreak-api.sequenceiq.com</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/cloudbreak>https://github.com/sequenceiq/cloudbreak</a></p>
<h3 id="cloudbreak-rest-client">Cloudbreak REST client</h3>
<p>GitHub: <a href=https://github.com/sequenceiq/cloudbreak-rest-client>https://github.com/sequenceiq/cloudbreak-rest-client</a></p>
<h3 id="cloudbreak-cli">Cloudbreak CLI</h3>
<p>GitHub: <a href=https://github.com/sequenceiq/cloudbreak-shell>https://github.com/sequenceiq/cloudbreak-shell</a></p>
<h3 id="cloudbreak-documentation">Cloudbreak documentation</h3>
<p>Product documentation: <a href=http://sequenceiq.com/cloudbreak>http://sequenceiq.com/cloudbreak</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/cloudbreak/blob/master/docs/index.md>https://github.com/sequenceiq/cloudbreak/blob/master/docs/index.md</a></p>
<p>API documentation: <a href=https://cloudbreak-api.sequenceiq.com/api/index.html>https://cloudbreak.sequenceiq.com/api</a></p>
<h3 id="ways-to-contribute">Ways to contribute</h3>
<ul>
<li>Use Cloudbreak</li>
<li>Submit a GitHub issue to the appropriate GitHub repository.</li>
<li>Submit a new feature request (as a GitHub issue).</li>
<li>Submit a code fix for a bug.</li>
<li>Submit a unit test.</li>
<li>Code review pending pull requests and bug fixes.</li>
<li>Tell others about these projects.</li>
</ul>
<h3 id="contributing-code">Contributing code</h3>
<p>We are always thrilled to receive pull requests, and do our best to process them as fast as possible. Not sure if that typo is worth a pull request? Do it! We will appreciate it.
The Cloudbreak projects are open source and developed/distributed under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache Software License, Version 2.0</a>.
If you wish to contribute to Cloudbreak (which you're very welcome and encouraged to do so) then you must agree to release the rights of your source under this license.</p>
<h4 id="creating-issues">Creating issues</h4>
<p>Any significant improvement should be documented as a GitHub issue before starting to work on it. Please use the appropriate labels - bug, enhancement, etc - this helps while creating the release notes for a version release.
Before submitting issues please check for duplicate or similar issues. If you are unclear about an issue please feel free to <a href="https://groups.google.com/forum/#!forum/cloudbreak">contact us</a>.</p>
<h4 id="discuss-your-design">Discuss your design</h4>
<p>We recommend discussing your plans on the <a href="https://groups.google.com/forum/#!forum/cloudbreak">mailing list</a> before starting to code - especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give feedback on your design, and maybe point out if someone else is working on the same thing.</p>
<h4 id="conventions">Conventions</h4>
<p>Please write clean code. Universally formatted code promotes ease of writing, reading, and maintenance.
* Do not use @author tags.</p>
<ul>
<li>
<p>New classes must match our dependency mechanism.</p>
</li>
<li>
<p>Code must be formatted according to our <a href="https://github.com/sequenceiq/cloudbreak/blob/master/config/eclipse_formatter.xml">formatter</a>.</p>
</li>
<li>
<p>Code must be checked with our <a href="https://github.com/sequenceiq/cloudbreak/tree/master/config/checkstyle">checkstyle</a>.</p>
</li>
<li>
<p>Contributions must pass existing unit tests.</p>
</li>
<li>
<p>The code changes must be accompanied by unit tests. In cases where unit tests are not possible or don’t make sense an explanation should be provided.</p>
</li>
<li>
<p>New unit tests should be provided to demonstrate bugs and fixes (use Mockito whenever possible).</p>
</li>
<li>
<p>The tests should be named *Test.java.</p>
</li>
<li>
<p>Use slf4j instead of commons logging as the logging facade.</p>
</li>
</ul>
<h3 id="thank-you">Thank you</h3>
<p>Huge thanks go to the contributors from the community who have been actively working with the SequenceIQ team. Kudos for that.</p>
<h3 id="jobs">Jobs</h3>
<p>Do you like what we are doing? We are looking for exceptional software engineers to join our development team. Would you like to work with others in the open source community?
Please consider submitting your resume and applying for open positions at jobs@sequenceiq.com.</p>
<h3 id="legal">Legal</h3>
<p><em>Brought to you courtesy of our legal counsel.</em></p>
<p>Use and transfer of Cloudbreak may be subject to certain restrictions by the
United States and other governments.<br />
It is your responsibility to ensure that your use and/or transfer does not
violate applicable laws.</p>
<h3 id="licensing">Licensing</h3>
<p>Cloudbreak is licensed under the Apache License, Version 2.0. See <a href="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/LICENSE">LICENSE</a> for the full license text.</p>
</div>
        </div>

        

        <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
        <script src="./js/bootstrap-3.0.3.min.js"></script>
        <script src="./js/prettify-1.0.min.js"></script>
        <script src="./js/base.js"></script>
    </body>
</html>